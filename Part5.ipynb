{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Global parameters for data generation\n",
    "L = 100  # Number of datasets\n",
    "N = 25  # Number of datapoints per dataset\n",
    "noise_std_dev = 0.3  # Standard deviation of Gaussian noise\n",
    "np.random.seed(42)  # for reproducibility\n",
    "\n",
    "\n",
    "# True underlying function\n",
    "def true_function(x):\n",
    "    return np.sin(2 * np.pi * x)\n",
    "\n",
    "\n",
    "print(f\"Parameters for Data Generation:\")\n",
    "print(f\"  Number of datasets (L): {L}\")\n",
    "print(f\"  Datapoints per dataset (N): {N}\")\n",
    "print(f\"  Noise standard deviation: {noise_std_dev}\")\n",
    "print(\"True function: sin(2 * pi * x)\")\n",
    "print(\"Imports and initial setup complete.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"--- Cell 2: Part (a) Generating Datasets ---\")\n",
    "\n",
    "datasets_x = []\n",
    "datasets_t = []  # t for target (noisy y)\n",
    "datasets_t_true = []  # true y for reference\n",
    "\n",
    "x_i_common = np.linspace(0, 1, N)  # Evenly spaced x values, same for all datasets\n",
    "\n",
    "for i in range(L):\n",
    "    t_true_i = true_function(x_i_common)\n",
    "    t_noisy_i = t_true_i + np.random.normal(0, noise_std_dev, N)\n",
    "\n",
    "    datasets_x.append(x_i_common)  # All x_i will be identical\n",
    "    datasets_t.append(t_noisy_i)\n",
    "    datasets_t_true.append(t_true_i)\n",
    "\n",
    "print(f\"Generated {L} datasets.\")\n",
    "print(\n",
    "    f\"Shape of datasets_x[0] (common x-values for all datasets): {datasets_x[0].shape}\"\n",
    ")\n",
    "print(\n",
    "    f\"Shape of datasets_t[0] (target values for the first dataset): {datasets_t[0].shape}\"\n",
    ")\n",
    "\n",
    "print(f\"\\nFirst 5 x-values (from datasets_x[0]):\")\n",
    "print(datasets_x[0][:5])\n",
    "print(f\"\\nFirst 5 true target values (from datasets_t_true[0]):\")\n",
    "print(datasets_t_true[0][:5])\n",
    "print(f\"\\nFirst 5 noisy target values (from datasets_t[0]):\")\n",
    "print(datasets_t[0][:5])\n",
    "\n",
    "\n",
    "# Plot one example dataset to verify\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(\n",
    "    datasets_x[0],\n",
    "    datasets_t[0],\n",
    "    label=f\"Noisy Datapoints (Dataset 0)\",\n",
    "    color=\"blue\",\n",
    "    s=50,\n",
    "    facecolors=\"none\",\n",
    ")\n",
    "x_plot_dense_true_func = np.linspace(0, 1, 200)\n",
    "plt.plot(\n",
    "    x_plot_dense_true_func,\n",
    "    true_function(x_plot_dense_true_func),\n",
    "    label=\"True Sinusoidal Function\",\n",
    "    color=\"green\",\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "plt.title(\"Example of a Generated Dataset\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"t\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\":\", alpha=0.7)\n",
    "plt.show()\n",
    "print(\"Example dataset plotted.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"--- Cell 3: Part (b) Defining Model (Gaussian Basis Functions) ---\")\n",
    "\n",
    "M_basis = 24  # Number of Gaussian basis functions\n",
    "M_total_params = M_basis + 1  # Total parameters M = M_basis + 1 (for the bias term w0)\n",
    "\n",
    "# Define Gaussian basis functions parameters\n",
    "s_gaussian = 0.1  # Width/scale of Gaussian basis functions (this is a hyperparameter)\n",
    "mus_gaussian = np.linspace(0, 1, M_basis)  # Centers of Gaussians\n",
    "\n",
    "print(f\"Model Parameters:\")\n",
    "print(f\"  Number of Gaussian basis functions (M_basis): {M_basis}\")\n",
    "print(\n",
    "    f\"  Total number of model parameters (M_total_params = M_basis + 1 bias): {M_total_params}\"\n",
    ")\n",
    "print(f\"  Width (scale 's') of Gaussian basis functions: {s_gaussian}\")\n",
    "print(f\"  First 5 Gaussian centers (mus): {mus_gaussian[:5]}\")\n",
    "if M_basis > 5:\n",
    "    print(f\"  Last 5 Gaussian centers (mus): {mus_gaussian[-5:]}\")\n",
    "\n",
    "\n",
    "def gaussian_basis_function(x, mu, s_param):\n",
    "    return np.exp(-0.5 * ((x - mu) / s_param) ** 2)\n",
    "\n",
    "\n",
    "def create_design_matrix(x_input, mus_list, scale_param):\n",
    "    N_points = len(x_input)\n",
    "    M_gaussians = len(mus_list)\n",
    "    phi = np.ones((N_points, M_gaussians + 1))\n",
    "    for j in range(M_gaussians):\n",
    "        phi[:, j + 1] = gaussian_basis_function(x_input, mus_list[j], scale_param)\n",
    "    return phi\n",
    "\n",
    "\n",
    "phi_example = create_design_matrix(datasets_x[0], mus_gaussian, s_gaussian)\n",
    "print(\n",
    "    f\"\\nExample Design Matrix (Phi) for the first dataset (N={N}, M_total_params={M_total_params}):\"\n",
    ")\n",
    "print(f\"  Shape of Phi: {phi_example.shape}\")\n",
    "assert phi_example.shape == (N, M_total_params), \"Design matrix shape is incorrect!\"\n",
    "print(f\"  First 3 rows and first 4 columns of Phi_example:\\n{phi_example[:3, :4]}\")\n",
    "print(\"Gaussian basis function and design matrix creation functions defined.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Cell 4: Part (c) Regularized Least Squares (Ridge Regression) ---\")\n",
    "\n",
    "\n",
    "def fit_regularized_least_squares(phi_matrix, t_vector, lambda_reg):\n",
    "    M_params = phi_matrix.shape[1]\n",
    "    identity_matrix = np.identity(M_params)\n",
    "    A = phi_matrix.T @ phi_matrix + lambda_reg * identity_matrix\n",
    "    b = phi_matrix.T @ t_vector\n",
    "    try:\n",
    "        weights = np.linalg.solve(A, b)\n",
    "    except np.linalg.LinAlgError:\n",
    "        print(\n",
    "            f\"Warning: Singular matrix encountered with lambda={lambda_reg}. Using pseudo-inverse.\"\n",
    "        )\n",
    "        weights = np.linalg.pinv(A) @ b\n",
    "    return weights\n",
    "\n",
    "\n",
    "def predict(phi_matrix, weights):\n",
    "    return phi_matrix @ weights\n",
    "\n",
    "\n",
    "print(\"Regularized least squares (fit and predict) functions defined.\")\n",
    "# Testing with dummy values (optional, but good for verifying function)\n",
    "dummy_phi = np.array([[1, 0.1, 0.2], [1, 0.5, 0.6], [1, 0.9, 1.0]])\n",
    "dummy_t = np.array([0.2, 0.8, 0.4])\n",
    "dummy_lambda = 0.1\n",
    "dummy_w = fit_regularized_least_squares(dummy_phi, dummy_t, dummy_lambda)\n",
    "print(\n",
    "    f\"Example: Dummy weights for a small test case (lambda={dummy_lambda}): {dummy_w}\"\n",
    ")\n",
    "print(\"Functions for fitting and prediction are ready.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Cell 6: Part (e) Choosing Regularization Coefficients (λ) ---\")\n",
    "\n",
    "lambda_values_chosen = [\n",
    "    1e-9,  # Very low lambda\n",
    "    1e-1,  # Medium lambda\n",
    "    1.0,  # High lambda\n",
    "]\n",
    "# Using simpler names for plot titles\n",
    "lambda_simple_names = [\n",
    "    f\"Low λ ({lambda_values_chosen[0]:.0e})\",\n",
    "    f\"Medium λ ({lambda_values_chosen[1]:.1f})\",\n",
    "    f\"High λ ({lambda_values_chosen[2]:.1f})\",\n",
    "]\n",
    "\n",
    "print(f\"Chosen Regularization Coefficients (lambda values):\")\n",
    "for i, val in enumerate(lambda_values_chosen):\n",
    "    print(f\"  {lambda_simple_names[i]}: {val}\")\n",
    "print(\"These lambda values will be used for fitting the model.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Cell 7: Part (f) & (g) Generating Plots for Bias-Variance Illustration ---\")\n",
    "\n",
    "x_plot_dense = np.linspace(0, 1, 200)\n",
    "phi_plot_dense = create_design_matrix(x_plot_dense, mus_gaussian, s_gaussian)\n",
    "print(\n",
    "    f\"Prepared dense x-values (shape: {x_plot_dense.shape}) and corresponding design matrix (shape: {phi_plot_dense.shape}) for plotting smooth curves.\"\n",
    ")\n",
    "\n",
    "all_predictions_by_lambda = {}\n",
    "\n",
    "for i, lambda_reg_val in enumerate(lambda_values_chosen):\n",
    "    current_lambda_name = lambda_simple_names[i]\n",
    "    print(f\"\\n--- Processing for: {current_lambda_name} (λ = {lambda_reg_val:.2e}) ---\")\n",
    "\n",
    "    predictions_for_current_lambda_list = []\n",
    "\n",
    "    # To print weights for one dataset (e.g., the first one) for this lambda\n",
    "    first_dataset_weights_printed = False\n",
    "\n",
    "    plt.figure(figsize=(12, 5))  # Figure for the two subplots for current lambda\n",
    "    plt.subplot(1, 2, 1)  # Left subplot: 100 estimated curves\n",
    "\n",
    "    for l_idx in range(L):  # Loop through each of the L datasets\n",
    "        x_train_dataset = datasets_x[l_idx]\n",
    "        t_train_dataset = datasets_t[l_idx]\n",
    "\n",
    "        phi_train_dataset = create_design_matrix(\n",
    "            x_train_dataset, mus_gaussian, s_gaussian\n",
    "        )\n",
    "        w_estimated = fit_regularized_least_squares(\n",
    "            phi_train_dataset, t_train_dataset, lambda_reg_val\n",
    "        )\n",
    "\n",
    "        if (\n",
    "            l_idx == 0 and not first_dataset_weights_printed\n",
    "        ):  # Print weights for the first dataset only\n",
    "            print(\n",
    "                f\"  Obtained model parameters (weights w) for Dataset 0 with {current_lambda_name}:\"\n",
    "            )\n",
    "            print(f\"    Shape of w: {w_estimated.shape}\")\n",
    "            print(f\"    First 5 weights (w0 is bias): {w_estimated[:5]}\")\n",
    "            first_dataset_weights_printed = True\n",
    "\n",
    "        y_pred_dense_single_fit = predict(phi_plot_dense, w_estimated)\n",
    "        predictions_for_current_lambda_list.append(y_pred_dense_single_fit)\n",
    "        plt.plot(x_plot_dense, y_pred_dense_single_fit, color=\"lightcoral\", alpha=0.15)\n",
    "\n",
    "    all_predictions_by_lambda[lambda_reg_val] = np.array(\n",
    "        predictions_for_current_lambda_list\n",
    "    )\n",
    "    print(\n",
    "        f\"  Stored {L} prediction curves for {current_lambda_name}. Shape: {all_predictions_by_lambda[lambda_reg_val].shape}\"\n",
    "    )\n",
    "\n",
    "    plt.plot(\n",
    "        x_plot_dense,\n",
    "        true_function(x_plot_dense),\n",
    "        color=\"green\",\n",
    "        linestyle=\"--\",\n",
    "        linewidth=2,\n",
    "        label=\"True Function h(x)\",\n",
    "    )\n",
    "    plt.title(f\"100 Estimated Curves\\n{current_lambda_name}\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y(x, w)\")\n",
    "    plt.ylim(-1.5, 1.5)\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.grid(True, linestyle=\":\", alpha=0.7)\n",
    "\n",
    "    plt.subplot(1, 2, 2)  # Right subplot: Mean and variance\n",
    "    mean_prediction_curve = np.mean(all_predictions_by_lambda[lambda_reg_val], axis=0)\n",
    "    std_dev_prediction_curves = np.std(\n",
    "        all_predictions_by_lambda[lambda_reg_val], axis=0\n",
    "    )\n",
    "\n",
    "    plt.plot(\n",
    "        x_plot_dense,\n",
    "        mean_prediction_curve,\n",
    "        color=\"red\",\n",
    "        linewidth=2,\n",
    "        label=\"Mean Prediction E[y(x,w)]\",\n",
    "    )\n",
    "    plt.plot(\n",
    "        x_plot_dense,\n",
    "        true_function(x_plot_dense),\n",
    "        color=\"green\",\n",
    "        linestyle=\"--\",\n",
    "        linewidth=2,\n",
    "        label=\"True Function h(x)\",\n",
    "    )\n",
    "    plt.fill_between(\n",
    "        x_plot_dense,\n",
    "        mean_prediction_curve - std_dev_prediction_curves,\n",
    "        mean_prediction_curve + std_dev_prediction_curves,\n",
    "        color=\"orangered\",\n",
    "        alpha=0.2,\n",
    "        label=\"Variance (±1 std. dev.)\",\n",
    "    )\n",
    "\n",
    "    plt.title(f\"Mean Prediction & Variance\\n{current_lambda_name}\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.ylim(-1.5, 1.5)\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.grid(True, linestyle=\":\", alpha=0.7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nAll plots generated and displayed.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (h) & (i): Bias-Variance Trade-off Observations\n",
    "\n",
    "**Recap of Key Concepts:**\n",
    "*   **Bias:** The difference between the average prediction of our model (the mean red curve in the right plots) and the true underlying function (green dashed curve). High bias means the model systematically misses the true function (underfitting).\n",
    "*   **Variance:** The variability of the model's predictions for a given input point if we were to train it on different datasets. Visually, this is represented by the spread of the light red curves in the left plots, and the width of the orange shaded band in the right plots. High variance means the model is very sensitive to the specific training data (overfitting).\n",
    "*   **Total Expected Error ≈ Bias² + Variance + Irreducible Error** (noise in data)\n",
    "*   **Regularization (λ):** A technique to control model complexity. Higher λ implies stronger regularization, penalizing large weights more.\n",
    "\n",
    "---\n",
    "\n",
    "### Observations for Each Regularization Level:\n",
    "\n",
    "(Refer to the plots and the printed outputs from Cell 7, e.g., the magnitude of weights for different lambdas)\n",
    "\n",
    "**1. Low λ (e.g., `1.00e-09`):**\n",
    "*   *Printed Weights (Dataset 0):* The weights (e.g., `w_estimated[:5]`) might show larger magnitudes compared to higher lambda values, reflecting the model's attempt to fit the data closely.\n",
    "*   **Plot 1 (100 Estimated Curves):**\n",
    "    *   *Observation:* The 100 individual estimated curves (light red) exhibit a **wide spread**. They wiggle significantly.\n",
    "    *   *Interpretation:* **HIGH VARIANCE**. The model is overfitting.\n",
    "*   **Plot 2 (Mean Estimated Curve & Variance):**\n",
    "    *   *Observation (Mean Curve):* The mean estimated curve (solid red) tracks the true sinusoidal function (green dashed line) **very closely**.\n",
    "    *   *Interpretation (Mean Curve):* **LOW BIAS**.\n",
    "    *   *Observation (Variance Band):* The shaded orange area is **wide**.\n",
    "    *   *Interpretation (Variance Band):* Confirms **HIGH VARIANCE**.\n",
    "*   **Summary for Low λ:** Highly flexible, low bias, high variance. The model fits noise.\n",
    "\n",
    "**2. Medium λ (e.g., `0.1`):**\n",
    "*   *Printed Weights (Dataset 0):* The magnitudes of weights (e.g., `w_estimated[:5]`) are likely smaller/more constrained than for low λ.\n",
    "*   **Plot 1 (100 Estimated Curves):**\n",
    "    *   *Observation:* The spread of the 100 individual curves is noticeably **narrower**.\n",
    "    *   *Interpretation:* **REDUCED VARIANCE**.\n",
    "*   **Plot 2 (Mean Estimated Curve & Variance):**\n",
    "    *   *Observation (Mean Curve):* The mean estimated curve (solid red) is **reasonably close** to the true function.\n",
    "    *   *Interpretation (Mean Curve):* **BIAS** might be slightly higher than for low λ, but relatively low.\n",
    "    *   *Observation (Variance Band):* The shaded orange area is **narrower**.\n",
    "    *   *Interpretation (Variance Band):* Shows **REDUCED VARIANCE**.\n",
    "*   **Summary for Medium λ:** Good balance. Regularization reduces variance with a slight potential increase in bias.\n",
    "\n",
    "**3. High λ (e.g., `1.0`):**\n",
    "*   *Printed Weights (Dataset 0):* The weights (e.g., `w_estimated[:5]`) are likely very small, with many close to zero, especially for the basis functions. The bias term `w0` might dominate if it's trying to fit the average of the sine wave.\n",
    "*   **Plot 1 (100 Estimated Curves):**\n",
    "    *   *Observation:* The 100 individual curves are **very tightly clustered** and much flatter.\n",
    "    *   *Interpretation:* **VERY LOW VARIANCE**.\n",
    "*   **Plot 2 (Mean Estimated Curve & Variance):**\n",
    "    *   *Observation (Mean Curve):* The mean estimated curve (solid red) is **noticeably different** and much simpler than the true function.\n",
    "    *   *Interpretation (Mean Curve):* **HIGH BIAS**. The model is underfitting.\n",
    "    *   *Observation (Variance Band):* The shaded orange area is **very narrow**.\n",
    "    *   *Interpretation (Variance Band):* Confirms **VERY LOW VARIANCE**.\n",
    "*   **Summary for High λ:** Over-regularized, low variance, high bias. Model is too simple.\n",
    "\n",
    "---\n",
    "\n",
    "### Describing the Bias-Variance Trade-off:\n",
    "\n",
    "The six plots and the behavior of the estimated weights for different λ values clearly demonstrate the bias-variance trade-off:\n",
    "\n",
    "*   **Low λ:** High model complexity allowed, leading to low bias (average fit is good) but high variance (fits vary wildly with data). Weights can be large.\n",
    "*   **High λ:** Low model complexity forced by regularization, leading to high bias (average fit is poor, too simple) but low variance (fits are very stable). Weights are small.\n",
    "*   **Medium λ:** Aims for a sweet spot where the model is complex enough to capture the true signal (moderate bias) but not so complex that it fits the noise (moderate variance). Weights are balanced.\n",
    "\n",
    "The parameters used (`L=100`, `N=25`, `M_total_params=25`, `noise_std_dev=0.3`, `s_gaussian=0.1`) create a scenario where overfitting is likely without regularization because the number of parameters is equal to the number of data points. Regularization (λ) is crucial for controlling this."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
