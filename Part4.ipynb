{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import (\n",
    "    shuffle as sk_shuffle,\n",
    ")\n",
    "\n",
    "def sigmoid(a):\n",
    "    return 1 / (1 + np.exp(-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_kernel_parameters(X_ref, kernelType, M_order):\n",
    "    \"\"\"\n",
    "    Generates parameters (centers mu, width s) for Gaussian/Sigmoidal kernels.\n",
    "    X_ref: Reference data (1D array, typically X_train) to determine range for mu.\n",
    "    kernelType: 'gaussian' or 'sigmoidal'.\n",
    "    M_order: Number of basis functions (e.g., Gaussians or sigmoids).\n",
    "    \"\"\"\n",
    "    params = {}\n",
    "    if kernelType == \"gaussian\" or kernelType == \"sigmoidal\":\n",
    "        if len(X_ref) == 0:\n",
    "            # Fallback if X_ref is empty\n",
    "            params[\"mu\"] = np.zeros(M_order) if M_order > 0 else []\n",
    "            params[\"s\"] = 0.1\n",
    "            return params\n",
    "\n",
    "        min_x, max_x = np.min(X_ref), np.max(X_ref)\n",
    "\n",
    "        if M_order > 0:\n",
    "            if min_x == max_x:  # All points in X_ref are the same\n",
    "                params[\"mu\"] = np.full(M_order, min_x)\n",
    "            else:\n",
    "                params[\"mu\"] = np.linspace(min_x, max_x, M_order)\n",
    "        else:  # M_order is 0\n",
    "            params[\"mu\"] = []\n",
    "\n",
    "        if M_order > 1 and min_x != max_x:\n",
    "            params[\"s\"] = (max_x - min_x) / M_order\n",
    "        elif M_order == 1 and min_x != max_x:\n",
    "            params[\"s\"] = (max_x - min_x) / 2.0 if (max_x - min_x) > 0 else 0.1\n",
    "        else:  # M_order is 0, or M_order >=1 but min_x == max_x\n",
    "            params[\"s\"] = 0.1  # Default small s\n",
    "\n",
    "        if params[\"s\"] == 0:  # Ensure s is not zero to avoid division by zero\n",
    "            params[\"s\"] = 0.1\n",
    "    return params\n",
    "\n",
    "def transform_features(X_input, kernelType, M_order, kernel_params):\n",
    "    \"\"\"\n",
    "    Transforms raw 1D input X_input into a design matrix Phi.\n",
    "    X_input: 1D numpy array of input samples.\n",
    "    kernelType: 'polynomial', 'gaussian', or 'sigmoidal'.\n",
    "    M_order: Polynomial degree or number of Gaussian/Sigmoidal basis functions.\n",
    "    kernel_params: Dictionary containing 'mu' and 's' for Gaussian/Sigmoidal.\n",
    "    Returns: Design matrix Phi (N_samples, M_order + 1).\n",
    "    \"\"\"\n",
    "    X_input = np.asarray(X_input).flatten()  # Ensure X_input is 1D\n",
    "    N = len(X_input)\n",
    "\n",
    "    if kernelType == \"polynomial\":\n",
    "        Phi = np.zeros((N, M_order + 1))\n",
    "        for i in range(M_order + 1):\n",
    "            Phi[:, i] = X_input**i\n",
    "    elif kernelType == \"gaussian\":\n",
    "        Phi = np.zeros((N, M_order + 1))\n",
    "        Phi[:, 0] = 1  # Bias term\n",
    "        if M_order > 0:\n",
    "            mu = kernel_params[\"mu\"]\n",
    "            s = kernel_params[\"s\"]\n",
    "            for j in range(M_order):\n",
    "                Phi[:, j + 1] = np.exp(-((X_input - mu[j]) ** 2) / (2 * s**2))\n",
    "    elif kernelType == \"sigmoidal\":\n",
    "        Phi = np.zeros((N, M_order + 1))\n",
    "        Phi[:, 0] = 1  # Bias term\n",
    "        if M_order > 0:\n",
    "            mu = kernel_params[\"mu\"]\n",
    "            s = kernel_params[\"s\"]\n",
    "            for j in range(M_order):\n",
    "                Phi[:, j + 1] = sigmoid((X_input - mu[j]) / s)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown kernel type: {kernelType}\")\n",
    "    return Phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to calculate prediction error (MSE)\n",
    "def ErrorPred(w_est, X_train, Y_train, X_test, Y_test, kernelType):\n",
    "    \"\"\"\n",
    "    (estimated weight, training data, training targets, testing data, testing targets, type of the kernel )\n",
    "    Computes and returns the train and test Mean Squared Errors (MSE).\n",
    "    \"\"\"\n",
    "    M = len(w_est) - 1  # M_order (degree or number of basis functions)\n",
    "    kernel_params = generate_kernel_parameters(X_train, kernelType, M)\n",
    "    Phi_train = transform_features(X_train, kernelType, M, kernel_params)\n",
    "    Phi_test = transform_features(X_test, kernelType, M, kernel_params)\n",
    "    Y_train_pred = Phi_train @ w_est\n",
    "    Y_test_pred = Phi_test @ w_est\n",
    "    TrainError = np.mean((Y_train.flatten() - Y_train_pred.flatten()) ** 2)\n",
    "    TestError = np.mean((Y_test.flatten() - Y_test_pred.flatten()) ** 2)\n",
    "\n",
    "    return TrainError, TestError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function for training using Stochastic Gradient Descent\n",
    "def Training(\n",
    "    X_train, Y_train, X_test, Y_test, kernelType, M_order, Epochs, BatchSize, stepSize\n",
    "):\n",
    "    \"\"\"\n",
    "    (training data, training targets, testing data, testing targets,\n",
    "     type of the kernel, order of the model, Number of epochs, Batch size, Step size)\n",
    "    Returns the estimated weights and histories of train/test errors.\n",
    "    \"\"\"\n",
    "    N_train = len(X_train)\n",
    "\n",
    "    # Initialize weights (M_order + 1 for bias)\n",
    "    weights = np.random.randn(M_order + 1) * 0.01\n",
    "\n",
    "    # Generate kernel parameters ONCE based on the full X_train\n",
    "    kernel_params_sgd = generate_kernel_parameters(X_train, kernelType, M_order)\n",
    "\n",
    "    train_errors_per_epoch = []\n",
    "    test_errors_per_epoch = []\n",
    "\n",
    "    print(\n",
    "        f\"Starting training: Kernel={kernelType}, M={M_order}, Epochs={Epochs}, BatchSize={BatchSize}, StepSize={stepSize}\"\n",
    "    )\n",
    "\n",
    "    for epoch in range(Epochs):\n",
    "        X_train_shuffled, Y_train_shuffled = sk_shuffle(\n",
    "            X_train, Y_train, random_state=epoch\n",
    "        )\n",
    "\n",
    "        num_batches = N_train // BatchSize\n",
    "        if N_train % BatchSize != 0:\n",
    "            num_batches += 1\n",
    "\n",
    "        for i in range(num_batches):\n",
    "            start_idx = i * BatchSize\n",
    "            end_idx = min((i + 1) * BatchSize, N_train)\n",
    "            X_batch = X_train_shuffled[start_idx:end_idx]\n",
    "            Y_batch = Y_train_shuffled[start_idx:end_idx]\n",
    "\n",
    "            if len(X_batch) == 0:\n",
    "                continue\n",
    "\n",
    "            batch_gradient_sum = np.zeros_like(weights)\n",
    "\n",
    "            for n in range(len(X_batch)):\n",
    "                x_n = X_batch[n]\n",
    "                y_n = Y_batch[n]\n",
    "                phi_n = transform_features(\n",
    "                    np.array([x_n]), kernelType, M_order, kernel_params_sgd\n",
    "                )[0]\n",
    "                y_pred_n = phi_n @ weights\n",
    "                grad_n = 2 * (y_pred_n - y_n) * phi_n\n",
    "                batch_gradient_sum += grad_n\n",
    "\n",
    "            mean_batch_gradient = batch_gradient_sum / len(X_batch)\n",
    "            weights -= stepSize * mean_batch_gradient\n",
    "\n",
    "        current_train_error, current_test_error = ErrorPred(\n",
    "            weights, X_train, Y_train, X_test, Y_test, kernelType\n",
    "        )\n",
    "        train_errors_per_epoch.append(current_train_error)\n",
    "        test_errors_per_epoch.append(current_test_error)\n",
    "\n",
    "        if (epoch + 1) % (Epochs // 10 if Epochs >= 10 else 1) == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch+1}/{Epochs} - Train MSE: {current_train_error:.4f}, Test MSE: {current_test_error:.4f}\"\n",
    "            )\n",
    "\n",
    "    # Plot training and testing error across the epochs\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(Epochs), train_errors_per_epoch, label=\"Training MSE\")\n",
    "    plt.plot(range(Epochs), test_errors_per_epoch, label=\"Testing MSE\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Mean Squared Error\")\n",
    "    plt.title(\n",
    "        f\"MSE vs. Epochs for {kernelType} (M={M_order}, BS={BatchSize}, LR={stepSize})\"\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return weights, train_errors_per_epoch, test_errors_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Pred(w_est, X_train, X_test, kernelType):\n",
    "    \"\"\"\n",
    "    (estimated weights, training data, testing data, type of the kernel )\n",
    "    Computes and returns the training and testing target estimates.\n",
    "    \"\"\"\n",
    "    M = len(w_est) - 1  # M_order\n",
    "    kernel_params = generate_kernel_parameters(X_train, kernelType, M)\n",
    "\n",
    "    Phi_train = transform_features(X_train, kernelType, M, kernel_params)\n",
    "    Phi_test = transform_features(X_test, kernelType, M, kernel_params)\n",
    "\n",
    "    Y_train_pred = Phi_train @ w_est\n",
    "    Y_test_pred = Phi_test @ w_est\n",
    "\n",
    "    return Y_train_pred, Y_test_pred\n",
    "\n",
    "\n",
    "def Pred_Error(w_est, X_train, Y_train, X_test, Y_test, kernelType):\n",
    "    \"\"\"\n",
    "    (estimated weights, training data, training targets, testing data, testing targets, type of the kernel )\n",
    "    Computes and returns the training and testing errors (MSE).\n",
    "    \"\"\"\n",
    "    return ErrorPred(w_est, X_train, Y_train, X_test, Y_test, kernelType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_data_3a(num_points=100, noise_std=0.1):\n",
    "    \"\"\"Generates data for Part 3(a): t_n = sin(2*pi*x_n) + e_n\"\"\"\n",
    "    X = np.random.rand(num_points, 1)\n",
    "    e = np.random.normal(0, noise_std, (num_points, 1))\n",
    "    T = np.sin(2 * np.pi * X) + e\n",
    "    return X.flatten(), T.flatten()\n",
    "\n",
    "\n",
    "def generate_data_3b(num_points_total=100, noise_std=0.1):\n",
    "    \"\"\"Generates data for Part 3(b): piecewise function\"\"\"\n",
    "    num_points_per_segment = num_points_total // 3\n",
    "    X_segments, T_segments = [], []\n",
    "\n",
    "    X1 = np.random.uniform(0, 1, num_points_per_segment)\n",
    "    T1 = np.sin(2 * np.pi * X1)\n",
    "    X_segments.append(X1)\n",
    "    T_segments.append(T1)\n",
    "\n",
    "    X2 = np.random.uniform(1, 2, num_points_per_segment)\n",
    "    T2 = 2 * (0.5 - np.abs(X2 - 1.5))\n",
    "    X_segments.append(X2)\n",
    "    T_segments.append(T2)\n",
    "\n",
    "    num_points_s3 = num_points_total - 2 * num_points_per_segment\n",
    "    X3 = np.random.uniform(2, 3, num_points_s3)\n",
    "    T3 = np.exp(-((X3 - 2.5) ** 2) / (2 * 0.1**2))\n",
    "    X_segments.append(X3)\n",
    "    T_segments.append(T3)\n",
    "\n",
    "    X = np.concatenate(X_segments)\n",
    "    T_true = np.concatenate(T_segments)\n",
    "    e = np.random.normal(0, noise_std, X.shape[0])\n",
    "    T = T_true + e\n",
    "\n",
    "    sorted_indices = np.argsort(X)\n",
    "    X = X[sorted_indices]\n",
    "    T = T[sorted_indices]\n",
    "\n",
    "    return X.flatten(), T.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "M_common = 5  # Order of model (polynomial degree or num basis functions)\n",
    "Epochs_common = 200\n",
    "BatchSize_common = 10\n",
    "StepSize_common = 0.01\n",
    "\n",
    "kernel_colors = {\"polynomial\": \"blue\", \"gaussian\": \"green\", \"sigmoidal\": \"red\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"----------------------------------------------------\")\n",
    "print(\"Part 4(a): Repeating 3(a) with SGD\")\n",
    "print(\"----------------------------------------------------\")\n",
    "\n",
    "print(\"\\n--- Data from 3(a) ---\")\n",
    "X_3a, T_3a = generate_data_3a(num_points=100)\n",
    "X_train_3a, X_test_3a, Y_train_3a, Y_test_3a = train_test_split(\n",
    "    X_3a, T_3a, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "results_3a = {}\n",
    "for kernel in [\"polynomial\", \"gaussian\", \"sigmoidal\"]:\n",
    "    print(f\"\\nTraining for 3(a) data with {kernel} kernel:\")\n",
    "    w_est_3a, _, _ = Training(\n",
    "        X_train_3a,\n",
    "        Y_train_3a,\n",
    "        X_test_3a,\n",
    "        Y_test_3a,\n",
    "        kernel,\n",
    "        M_common,\n",
    "        Epochs_common,\n",
    "        BatchSize_common,\n",
    "        StepSize_common,\n",
    "    )\n",
    "\n",
    "    final_train_err, final_test_err = Pred_Error(\n",
    "        w_est_3a, X_train_3a, Y_train_3a, X_test_3a, Y_test_3a, kernel\n",
    "    )\n",
    "    print(\n",
    "        f\"Final MSE for {kernel} (3a data) - Train: {final_train_err:.4f}, Test: {final_test_err:.4f}\"\n",
    "    )\n",
    "\n",
    "    Y_pred_train_3a, Y_pred_test_3a = Pred(w_est_3a, X_train_3a, X_test_3a, kernel)\n",
    "    results_3a[kernel] = {\n",
    "        \"weights\": w_est_3a,\n",
    "        \"train_err\": final_train_err,\n",
    "        \"test_err\": final_test_err,\n",
    "        \"Y_pred_train\": Y_pred_train_3a,\n",
    "        \"Y_pred_test\": Y_pred_test_3a,\n",
    "    }\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sort_idx_train = np.argsort(X_train_3a)\n",
    "    plt.scatter(\n",
    "        X_train_3a[sort_idx_train],\n",
    "        Y_train_3a[sort_idx_train],\n",
    "        label=\"True Train Data\",\n",
    "        alpha=0.6,\n",
    "        s=10,\n",
    "    )\n",
    "    plt.plot(\n",
    "        X_train_3a[sort_idx_train],\n",
    "        Y_pred_train_3a[sort_idx_train],\n",
    "        color=kernel_colors[kernel],\n",
    "        label=f\"{kernel} Pred (Train)\",\n",
    "    )\n",
    "    plt.title(f\"3(a) Train Data vs {kernel} Prediction\")\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"T\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sort_idx_test = np.argsort(X_test_3a)\n",
    "    plt.scatter(\n",
    "        X_test_3a[sort_idx_test],\n",
    "        Y_test_3a[sort_idx_test],\n",
    "        label=\"True Test Data\",\n",
    "        alpha=0.6,\n",
    "        s=10,\n",
    "    )\n",
    "    plt.plot(\n",
    "        X_test_3a[sort_idx_test],\n",
    "        Y_pred_test_3a[sort_idx_test],\n",
    "        color=kernel_colors[kernel],\n",
    "        label=f\"{kernel} Pred (Test)\",\n",
    "    )\n",
    "    plt.title(f\"3(a) Test Data vs {kernel} Prediction\")\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"T\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n----------------------------------------------------\")\n",
    "print(\"Part 4(a): Repeating 3(b) with SGD\")\n",
    "print(\"----------------------------------------------------\")\n",
    "\n",
    "print(\"\\n--- Data from 3(b) ---\")\n",
    "X_3b, T_3b = generate_data_3b(num_points_total=150)\n",
    "X_train_3b, X_test_3b, Y_train_3b, Y_test_3b = train_test_split(\n",
    "    X_3b, T_3b, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "results_3b = {}\n",
    "for kernel in [\"polynomial\", \"gaussian\", \"sigmoidal\"]:\n",
    "    print(f\"\\nTraining for 3(b) data with {kernel} kernel:\")\n",
    "    w_est_3b, _, _ = Training(\n",
    "        X_train_3b,\n",
    "        Y_train_3b,\n",
    "        X_test_3b,\n",
    "        Y_test_3b,\n",
    "        kernel,\n",
    "        M_common,\n",
    "        Epochs_common,\n",
    "        BatchSize_common,\n",
    "        StepSize_common,\n",
    "    )\n",
    "\n",
    "    final_train_err, final_test_err = Pred_Error(\n",
    "        w_est_3b, X_train_3b, Y_train_3b, X_test_3b, Y_test_3b, kernel\n",
    "    )\n",
    "    print(\n",
    "        f\"Final MSE for {kernel} (3b data) - Train: {final_train_err:.4f}, Test: {final_test_err:.4f}\"\n",
    "    )\n",
    "\n",
    "    Y_pred_train_3b, Y_pred_test_3b = Pred(w_est_3b, X_train_3b, X_test_3b, kernel)\n",
    "    results_3b[kernel] = {\n",
    "        \"weights\": w_est_3b,\n",
    "        \"train_err\": final_train_err,\n",
    "        \"test_err\": final_test_err,\n",
    "        \"Y_pred_train\": Y_pred_train_3b,\n",
    "        \"Y_pred_test\": Y_pred_test_3b,\n",
    "    }\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sort_idx_train = np.argsort(X_train_3b)\n",
    "    plt.scatter(\n",
    "        X_train_3b[sort_idx_train],\n",
    "        Y_train_3b[sort_idx_train],\n",
    "        label=\"True Train Data\",\n",
    "        alpha=0.6,\n",
    "        s=10,\n",
    "    )\n",
    "    plt.plot(\n",
    "        X_train_3b[sort_idx_train],\n",
    "        Y_pred_train_3b[sort_idx_train],\n",
    "        color=kernel_colors[kernel],\n",
    "        label=f\"{kernel} Pred (Train)\",\n",
    "    )\n",
    "    plt.title(f\"3(b) Train Data vs {kernel} Prediction\")\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"T\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sort_idx_test = np.argsort(X_test_3b)\n",
    "    plt.scatter(\n",
    "        X_test_3b[sort_idx_test],\n",
    "        Y_test_3b[sort_idx_test],\n",
    "        label=\"True Test Data\",\n",
    "        alpha=0.6,\n",
    "        s=10,\n",
    "    )\n",
    "    plt.plot(\n",
    "        X_test_3b[sort_idx_test],\n",
    "        Y_pred_test_3b[sort_idx_test],\n",
    "        color=kernel_colors[kernel],\n",
    "        label=f\"{kernel} Pred (Test)\",\n",
    "    )\n",
    "    plt.title(f\"3(b) Test Data vs {kernel} Prediction\")\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"T\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n----------------------------------------------------\")\n",
    "print(\"Part 4(a): Study the effect of stepSize\")\n",
    "print(\"----------------------------------------------------\")\n",
    "\n",
    "kernel_for_lr_study = \"gaussian\"  # Using Gaussian kernel and 3(a) data as example\n",
    "step_sizes_to_test = [0.1, 0.01, 0.001, 0.0001]\n",
    "lr_study_results_test_mse = {}\n",
    "\n",
    "for lr in step_sizes_to_test:\n",
    "    print(f\"\\nTraining with stepSize = {lr} (Kernel: {kernel_for_lr_study}, Data: 3a)\")\n",
    "    # For this study, we only need the error history, so we call Training and use its returned history\n",
    "    # We suppress the individual plots from Training by modifying it or just focusing on its returned values\n",
    "\n",
    "    # Re-using X_train_3a, Y_train_3a, X_test_3a, Y_test_3a\n",
    "    # The Training function already plots, but if we want a combined plot, we'd need to capture histories\n",
    "\n",
    "    # Simplified Training loop to capture history for this specific study (without individual plots)\n",
    "    weights_lr = np.random.randn(M_common + 1) * 0.01\n",
    "    kernel_params_lr = generate_kernel_parameters(\n",
    "        X_train_3a, kernel_for_lr_study, M_common\n",
    "    )\n",
    "    current_lr_test_mse_history = []\n",
    "\n",
    "    for epoch in range(Epochs_common):\n",
    "        X_train_shuffled, Y_train_shuffled = sk_shuffle(\n",
    "            X_train_3a, Y_train_3a, random_state=epoch\n",
    "        )\n",
    "        num_batches = len(X_train_shuffled) // BatchSize_common\n",
    "        if len(X_train_shuffled) % BatchSize_common != 0:\n",
    "            num_batches += 1\n",
    "\n",
    "        for i in range(num_batches):\n",
    "            start_idx = i * BatchSize_common\n",
    "            end_idx = min((i + 1) * BatchSize_common, len(X_train_shuffled))\n",
    "            X_batch = X_train_shuffled[start_idx:end_idx]\n",
    "            Y_batch = Y_train_shuffled[start_idx:end_idx]\n",
    "            if len(X_batch) == 0:\n",
    "                continue\n",
    "\n",
    "            batch_gradient_sum = np.zeros_like(weights_lr)\n",
    "            for n_idx in range(len(X_batch)):\n",
    "                phi_n = transform_features(\n",
    "                    np.array([X_batch[n_idx]]),\n",
    "                    kernel_for_lr_study,\n",
    "                    M_common,\n",
    "                    kernel_params_lr,\n",
    "                )[0]\n",
    "                y_pred_n = phi_n @ weights_lr\n",
    "                grad_n = 2 * (y_pred_n - Y_batch[n_idx]) * phi_n\n",
    "                batch_gradient_sum += grad_n\n",
    "            mean_batch_gradient = batch_gradient_sum / len(X_batch)\n",
    "            weights_lr -= lr * mean_batch_gradient\n",
    "\n",
    "        _, test_mse_epoch = ErrorPred(\n",
    "            weights_lr,\n",
    "            X_train_3a,\n",
    "            Y_train_3a,\n",
    "            X_test_3a,\n",
    "            Y_test_3a,\n",
    "            kernel_for_lr_study,\n",
    "        )\n",
    "        current_lr_test_mse_history.append(test_mse_epoch)\n",
    "        if (epoch + 1) % (Epochs_common // 10 if Epochs_common >= 10 else 1) == 0:\n",
    "            print(f\"LR={lr}, Epoch {epoch+1}, Test MSE: {test_mse_epoch:.4f}\")\n",
    "    lr_study_results_test_mse[lr] = current_lr_test_mse_history\n",
    "\n",
    "# Plotting Test MSE vs Epochs for different learning rates\n",
    "plt.figure(figsize=(10, 6))\n",
    "for lr, mse_history in lr_study_results_test_mse.items():\n",
    "    plt.plot(range(Epochs_common), mse_history, label=f\"Step Size = {lr}\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Test MSE\")\n",
    "plt.title(f\"Effect of Step Size on Convergence ({kernel_for_lr_study} kernel, 3a data)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "# Dynamically adjust ylim based on observed MSEs, capped for readability\n",
    "max_mse_observed = 0\n",
    "for hist in lr_study_results_test_mse.values():\n",
    "    max_mse_observed = max(max_mse_observed, max(hist) if hist else 0)\n",
    "plt.ylim(0, min(1.0, max_mse_observed * 1.2 if max_mse_observed > 0 else 1.0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Part 4(b) - Study effect of BatchSize\n",
    "\n",
    "print(\"\\n----------------------------------------------------\")\n",
    "print(\"Part 4(b): Study the effect of batchSize\")\n",
    "print(\"----------------------------------------------------\")\n",
    "\n",
    "kernel_for_bs_study = \"gaussian\"\n",
    "step_size_for_bs_study = 0.01  # Assuming this was a good one from LR study\n",
    "# Ensure batch sizes are valid for the dataset size\n",
    "valid_batch_sizes = [1, 10]\n",
    "if len(X_train_3a) // 2 > 0:\n",
    "    valid_batch_sizes.append(len(X_train_3a) // 2)\n",
    "if len(X_train_3a) > 0:\n",
    "    valid_batch_sizes.append(len(X_train_3a))\n",
    "batch_sizes_to_test = sorted(list(set(valid_batch_sizes)))  # Unique, sorted\n",
    "\n",
    "bs_study_results_test_mse = {}\n",
    "\n",
    "for bs in batch_sizes_to_test:\n",
    "    if bs == 0:\n",
    "        continue\n",
    "    print(\n",
    "        f\"\\nTraining with BatchSize = {bs} (Kernel: {kernel_for_bs_study}, LR={step_size_for_bs_study}, Data: 3a)\"\n",
    "    )\n",
    "\n",
    "    weights_bs = np.random.randn(M_common + 1) * 0.01\n",
    "    kernel_params_bs = generate_kernel_parameters(\n",
    "        X_train_3a, kernel_for_bs_study, M_common\n",
    "    )\n",
    "    current_bs_test_mse_history = []\n",
    "\n",
    "    for epoch in range(Epochs_common):\n",
    "        X_train_shuffled, Y_train_shuffled = sk_shuffle(\n",
    "            X_train_3a, Y_train_3a, random_state=epoch\n",
    "        )\n",
    "        num_batches = len(X_train_shuffled) // bs\n",
    "        if len(X_train_shuffled) % bs != 0:\n",
    "            num_batches += 1\n",
    "\n",
    "        for i in range(num_batches):\n",
    "            start_idx = i * bs\n",
    "            end_idx = min((i + 1) * bs, len(X_train_shuffled))\n",
    "            X_batch = X_train_shuffled[start_idx:end_idx]\n",
    "            Y_batch = Y_train_shuffled[start_idx:end_idx]\n",
    "            if len(X_batch) == 0:\n",
    "                continue\n",
    "\n",
    "            batch_gradient_sum = np.zeros_like(weights_bs)\n",
    "            for n_idx in range(len(X_batch)):\n",
    "                phi_n = transform_features(\n",
    "                    np.array([X_batch[n_idx]]),\n",
    "                    kernel_for_bs_study,\n",
    "                    M_common,\n",
    "                    kernel_params_bs,\n",
    "                )[0]\n",
    "                y_pred_n = phi_n @ weights_bs\n",
    "                grad_n = 2 * (y_pred_n - Y_batch[n_idx]) * phi_n\n",
    "                batch_gradient_sum += grad_n\n",
    "            mean_batch_gradient = batch_gradient_sum / len(X_batch)\n",
    "            weights_bs -= step_size_for_bs_study * mean_batch_gradient\n",
    "\n",
    "        _, test_mse_epoch = ErrorPred(\n",
    "            weights_bs,\n",
    "            X_train_3a,\n",
    "            Y_train_3a,\n",
    "            X_test_3a,\n",
    "            Y_test_3a,\n",
    "            kernel_for_bs_study,\n",
    "        )\n",
    "        current_bs_test_mse_history.append(test_mse_epoch)\n",
    "        if (epoch + 1) % (Epochs_common // 10 if Epochs_common >= 10 else 1) == 0:\n",
    "            print(f\"BS={bs}, Epoch {epoch+1}, Test MSE: {test_mse_epoch:.4f}\")\n",
    "    bs_study_results_test_mse[bs] = current_bs_test_mse_history\n",
    "\n",
    "# Plotting Test MSE vs Epochs for different batch sizes\n",
    "plt.figure(figsize=(10, 6))\n",
    "for bs, mse_history in bs_study_results_test_mse.items():\n",
    "    label_bs = f\"Batch Size = {bs}\"\n",
    "    if bs == 1:\n",
    "        label_bs += \" (SGD)\"\n",
    "    if bs == len(X_train_3a) and len(X_train_3a) > 0:\n",
    "        label_bs += \" (Full Batch)\"\n",
    "    plt.plot(range(Epochs_common), mse_history, label=label_bs)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Test MSE\")\n",
    "plt.title(\n",
    "    f\"Effect of Batch Size on Convergence ({kernel_for_bs_study} kernel, 3a data, LR={step_size_for_bs_study})\"\n",
    ")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "max_mse_observed_bs = 0\n",
    "for hist in bs_study_results_test_mse.values():\n",
    "    max_mse_observed_bs = max(max_mse_observed_bs, max(hist) if hist else 0)\n",
    "plt.ylim(0, min(1.0, max_mse_observed_bs * 1.2 if max_mse_observed_bs > 0 else 1.0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 12: Part 4(c) - Discussion and Observations\n",
    "\n",
    "**Observations from Part 4(a) - SGD on 3(a) and 3(b) data:**\n",
    "\n",
    "*   **General Performance:** Stochastic Gradient Descent (SGD), when tuned appropriately (learning rate, batch size, epochs), can effectively find good solutions for kernel regression problems. The convergence plots (Mean Squared Error vs. Epochs) generally show a decreasing error trend, ideally stabilizing at a low value.\n",
    "*   **Kernel Suitability for 3(a) (Sinusoidal data `t_n = sin(2*pi*x_n) + e_n`):**\n",
    "    *   *Polynomial Kernels:* Can approximate sine waves, especially with a sufficient degree M (e.g., M=5 or higher). However, polynomials are global and can exhibit undesirable oscillations outside the training data range (poor extrapolation). They might struggle if the sine wave has high frequency relative to M.\n",
    "    *   *Gaussian Kernels:* Very flexible and well-suited for smooth functions like sinusoids. With an appropriate number of basis functions (M) and a suitable width parameter 's', they can model the sine wave accurately. The even spacing of Gaussian centers (`mu_j`) across the data range helps capture the periodic nature.\n",
    "    *   *Sigmoidal Kernels:* Can also model non-linearities. A sum of sigmoids can approximate various functions. While they might be less \"naturally\" shaped for a simple sinusoid compared to Gaussians, they can still perform adequately with enough basis functions.\n",
    "*   **Kernel Suitability for 3(b) (Piecewise data):** This dataset is more challenging due to its non-smooth, piecewise nature (sinusoid -> triangle -> Gaussian peak).\n",
    "    *   *Polynomial Kernels:* Likely to perform poorly. A single polynomial of moderate degree M struggles to fit sharp transitions and distinctly different functional forms across the entire domain [0,3]. High M might lead to severe overfitting and wild oscillations (Runge's phenomenon).\n",
    "    *   *Gaussian Kernels:* Generally perform better than polynomials for this type of data due to their local nature. A sum of Gaussians can approximate piecewise functions if M is large enough and the centers (`mu_j`) and widths (`s`) are well-chosen or adapted. They tend to smooth out the sharp corners but can capture the overall shapes of the segments.\n",
    "    *   *Sigmoidal Kernels:* Similar to Gaussians, a sum of sigmoids can approximate complex, piecewise functions. Their step-like nature can be somewhat beneficial for modeling transitions, but like Gaussians, they will smooth sharp corners.\n",
    "*   **Comparison to Closed-Form (qualitatively, based on typical Part 3 results):**\n",
    "    *   SGD solutions, if converged properly, should yield Mean Squared Errors (MSEs) comparable to those obtained from closed-form solutions (e.g., `w = (Phi^T Phi + lambda I)^-1 Phi^T T`).\n",
    "    *   Closed-form solutions provide the exact optimal weights for a given feature transformation (kernel and M) and regularization, assuming the matrix inversion is feasible. SGD is an iterative approximation.\n",
    "    *   SGD might get stuck in local minima if the loss surface is non-convex (though for linear regression on fixed features, the MSE loss is convex w.r.t. weights). The primary advantage of SGD is its scalability to very large datasets where forming and inverting `(Phi^T Phi)` is computationally prohibitive or memory-intensive.\n",
    "    *   Convergence of SGD to the global minimum (same as closed-form without regularization or with L2 and convex loss) depends on step size, batch size, and number of epochs.\n",
    "\n",
    "**Observations from Part 4(a) - Effect of `stepSize` (Learning Rate) on Convergence:**\n",
    "\n",
    "*   **Too Large `stepSize` (e.g., 0.1 in the plots):** Leads to divergence or unstable oscillations. The MSE might increase rapidly or fluctuate wildly without settling. This is because the weight updates overshoot the optimal values.\n",
    "*   **Too Small `stepSize` (e.g., 0.0001 in the plots):** Convergence is very slow. The MSE decreases, but it takes a very large number of epochs to reach a good solution. The updates are too cautious.\n",
    "*   **Appropriate `stepSize` (e.g., 0.01 or 0.001 in the plots for the chosen M, BatchSize):** Results in stable and reasonably fast convergence towards a low MSE. There's often a \"sweet spot\" or a range of good learning rates.\n",
    "*   The optimal `stepSize` is highly dependent on the dataset characteristics, the chosen kernel, the model complexity (M_order), and importantly, the `BatchSize`.\n",
    "\n",
    "**Observations from Part 4(b) - Effect of `BatchSize` on Speed of Convergence:**\n",
    "\n",
    "*   **Small `BatchSize` (e.g., 1, which is true Stochastic Gradient Descent):**\n",
    "    *   *Pros:* Each epoch involves many small, frequent updates. The high variance in gradients can help escape shallow local minima. Computation per update is minimal.\n",
    "    *   *Cons:* The convergence path is very noisy (MSE fluctuates significantly epoch to epoch). It often requires a smaller learning rate for stability, which can slow down overall convergence in terms of epochs to reach a target MSE.\n",
    "*   **Large `BatchSize` (e.g., `len(X_train)`, which is Full Batch Gradient Descent):**\n",
    "    *   *Pros:* Gradients are computed over the entire training set, providing a more accurate estimate of the true gradient. This leads to a smoother convergence path. Often allows for larger learning rates.\n",
    "    *   *Cons:* Each update (and thus each epoch) is computationally expensive, especially for large datasets. Can sometimes get stuck in sharper local minima more easily than SGD.\n",
    "*   **Mini-batch (e.g., 10, `len(X_train)//2`):**\n",
    "    *   *Pros:* Offers a balance. Reduces the variance of gradient estimates compared to SGD, leading to more stable convergence. More computationally efficient per epoch than full-batch. Often leverages vectorized operations well, leading to good wall-clock time performance.\n",
    "    *   *Cons:* Introduces `BatchSize` as another hyperparameter to tune.\n",
    "*   **Speed of Convergence (Epochs vs. Wall-Clock Time):**\n",
    "    *   *Epochs:* Full batch might appear to converge in fewer epochs if the y-axis (MSE) stabilizes quickly. However, each epoch takes much longer.\n",
    "    *   *Wall-clock time:* Mini-batch SGD often provides the best trade-off and converges fastest in terms of actual time taken to reach a desired error level, especially on large datasets. True SGD (batch_size=1) can be slow in wall-clock time if individual updates are not well-vectorized or if data loading is a bottleneck, despite fast epochs.\n",
    "    *   In the plots, we see that `Batch Size = 1` is very noisy. `Batch Size = 10` (mini-batch) shows a good balance. Full batch is smooth but might be slow if epochs were very long.\n",
    "\n",
    "**Part 4(c): Suggestions on How to Choose `stepSize` and `BatchSize` for a New Problem:**\n",
    "\n",
    "1.  **Start with `BatchSize`:**\n",
    "    *   **Common Choices:** Powers of 2 like 32, 64, 128, 256 are often good starting points for mini-batches due to hardware optimizations.\n",
    "    *   **Dataset Size:**\n",
    "        *   For small datasets (e.g., < 2000 samples), full-batch might be feasible and simpler (no `BatchSize` tuning needed, potentially more stable gradients).\n",
    "        *   For very large datasets, smaller mini-batches are necessary due to memory constraints and for faster iteration.\n",
    "    *   **Experiment:** If unsure, try a few common values (e.g., a small mini-batch like 32, a larger one like 256, and perhaps full-batch if feasible) and see how training behaves.\n",
    "\n",
    "2.  **Tune `stepSize` (Learning Rate):**\n",
    "    *   This is often the **most critical hyperparameter**.\n",
    "    *   **Initial Range:** Try values spanning several orders of magnitude, e.g., `[0.1, 0.03, 0.01, 0.003, 0.001, 0.0001]`.\n",
    "    *   **Observation Strategy:**\n",
    "        *   **Loss Diverges/Explodes:** `stepSize` is too high. Reduce it significantly (e.g., by a factor of 3-10).\n",
    "        *   **Loss Oscillates Wildly but Converges Slowly:** `stepSize` might still be too high, or you might benefit from a learning rate schedule.\n",
    "        *   **Loss Decreases Very Slowly:** `stepSize` is too low. Increase it (e.g., by a factor of 3-10).\n",
    "        *   **Loss Plateaus Quickly at a High Value:** `stepSize` might be too low, or the model lacks capacity, or it's stuck in a poor local minimum.\n",
    "    *   **Plot Loss:** Always plot the training (and validation) loss vs. epochs. This is the best way to diagnose `stepSize` issues.\n",
    "    *   **Relationship with `BatchSize`:** Smaller batch sizes generally require smaller learning rates for stable training due to higher variance in gradient estimates. If you decrease `BatchSize`, you might need to decrease `stepSize`. Conversely, if you increase `BatchSize`, you might be able to use a larger `stepSize`.\n",
    "\n",
    "3.  **Iterate and Refine:**\n",
    "    *   The optimal `stepSize` and `BatchSize` can be interdependent. After finding a reasonable `BatchSize`, fine-tune the `stepSize`. Then, you might revisit `BatchSize` if convergence is still problematic.\n",
    "    *   Consider learning rate schedules (e.g., reducing `stepSize` after a certain number of epochs or when loss plateaus) for more advanced tuning. Adam or RMSprop optimizers adapt learning rates automatically and are often good defaults.\n",
    "\n",
    "4.  **Monitor Validation Error:**\n",
    "    *   **Crucially**, tune these hyperparameters based on performance on a **separate validation set**, not just the training set error. This helps prevent overfitting to the training data and ensures the model generalizes well.\n",
    "    *   The goal is to find settings that minimize validation error, not necessarily training error (which can be driven to zero by overfitting).\n",
    "\n",
    "5.  **Computational Budget:**\n",
    "    *   The choice also depends on your computational resources and time constraints. Full batch might give smooth convergence but be too slow per epoch. Small mini-batches might allow for rapid iteration and experimentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
